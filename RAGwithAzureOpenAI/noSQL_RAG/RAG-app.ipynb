{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CosmosDB for NoSQL RAG\n",
    "<img src = \"./cosmosdbrag.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing important packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-cosmos\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cosmos import CosmosClient, PartitionKey, exceptions\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a connection to CosmosDB via connection string\n",
    "and creating a database if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmosdb_connection_string = os.getenv(\"COSMOSDB_CONNECTION_STRING\")\n",
    "\n",
    "client = CosmosClient.from_connection_string(cosmosdb_connection_string)\n",
    "database_name = os.getenv(\"DATABASE_NAME\")\n",
    "\n",
    "database = client.create_database_if_not_exists(id=database_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the vector embedding policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk = \"/category\"\n",
    "\n",
    "vector_embedding_policy = {\n",
    "    \"vectorEmbeddings\": [\n",
    "        {\n",
    "            \"path\":\"/vector\",\n",
    "            \"dataType\":\"float32\",\n",
    "            \"distanceFunction\":\"cosine\",\n",
    "            \"dimensions\":1536\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a vector index with diskANN algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_policy = {\n",
    "    \"vectorIndexes\": [\n",
    "        {\n",
    "            \"path\":\"/vector\",\n",
    "            \"type\":\"diskANN\"\n",
    "        }\n",
    "\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating container inside of the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    container_name = os.getenv(\"CONTAINER_NAME\")\n",
    "    \n",
    "    container = database.create_container_if_not_exists(\n",
    "        id=container_name,\n",
    "        partition_key=PartitionKey(path=pk),\n",
    "        indexing_policy=indexing_policy,\n",
    "        vector_embedding_policy=vector_embedding_policy\n",
    "        \n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Azure OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "\n",
    "azure_openai_client = AzureOpenAI(\n",
    "    api_key=azure_openai_key,\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    azure_endpoint=azure_openai_endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Embedding Generation Function\n",
    "embedding engine to be used: text-embedding-ada-002 \n",
    "<br>\n",
    "vector dimensions: 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(client, text):\n",
    "    embedding_model = os.getenv(\"EMBEDDING_ENGINE\")\n",
    "    \n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model = embedding_model\n",
    "    )\n",
    "    \n",
    "    embeddings=response.model_dump()\n",
    "    return embeddings['data'][0]['embedding']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading food dataset\n",
    "the food dataset is stored in `\"./fooditems.json\"`\n",
    "<br>\n",
    "we will generate vector embedding for the `/description` field of each food object and store it in a new field `/vector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "\n",
    "file_path = \"./food_items.json\"\n",
    "\n",
    "with open(file_path) as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "\n",
    "    \n",
    "for obj in data:\n",
    "    guid = str(uuid.uuid4())\n",
    "    vector_embeddings = generate_embeddings(azure_openai_client, obj['description'])\n",
    "    obj['vector'] = vector_embeddings\n",
    "    obj['id']=guid\n",
    "    container.upsert_item(obj)\n",
    "    \n",
    "    \n",
    "with open(\"./new_dataset.json\", 'w') as f:\n",
    "    json.dump(data, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating vector embeddings for the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query=\"are pizzas available? i am lactose intolerant\"\n",
    "user_embeddings = generate_embeddings(azure_openai_client, user_query)\n",
    "print(user_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending a query to database with filtering based upon VectorDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryText = f\"\"\" SELECT TOP 5 c.category, c.name, c.description, c.price, VectorDistance(c.vector, {user_embeddings}) AS SimilarityScore\n",
    "FROM c\n",
    "ORDER BY VectorDistance(c.vector, {user_embeddings})\"\"\"\n",
    "results = container.query_items(\n",
    "    query=queryText,\n",
    "    enable_cross_partition_query=True\n",
    ")\n",
    "dishes = []\n",
    "\n",
    "for item in results:\n",
    "    print(item)\n",
    "    dishes.append(item)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending call to our GPT engine for summarisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = f\"\"\"You are meant to behave as a RAG chatbot that derives its context from a database of food items stored in azure cosmosDB for noSQL API.\n",
    "please asnwer strictly from the context from the database provided and if you dont have an answer please politely say so. dont include any extra \n",
    "information that is not in the context and dont include links as well.\n",
    "the context passed to you will be in the form of a pythonic list with each object in the list containing details of a food item and\n",
    "having structure as follows:\n",
    "\n",
    " \"category\": \"the category of the food item like smoothies, burgers, etc\",\n",
    " \"name\": \"the name of the food item\",\n",
    " \"description\": \"the description of the food item\",\n",
    "\"price\": \"the price of the food item in USD\",\n",
    "\n",
    "\n",
    "the pythonic list contains best 5 matches to the user query based on cosine similarity of the embeddings of the user query and the food item descriptions.\n",
    "please structure your answers in a very professional manner and in such a way that the user does not get to know that its RAG working under the hood\n",
    "and its as if they are talking to a human.\"\"\"\n",
    "\n",
    "user_message = f\"\"\" the user query is: {user_query}\n",
    "the context is : {dishes}\"\"\"\n",
    "\n",
    "chat_completions_response = azure_openai_client.chat.completions.create(\n",
    "    model = os.getenv(\"GPT_ENGINE\"),\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(chat_completions_response.choices[0].message.content)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
